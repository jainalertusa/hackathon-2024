{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MnOW6nOsOlwV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "directory = '/usr/realestate data'\n",
        "\n",
        "# Initialize an empty DataFrame to hold the merged data\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        df = pd.read_csv(file_path)\n",
        "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
        "\n",
        "# Save the merged DataFrame to a new CSV file\n",
        "merged_df.to_csv('merged_data.csv', index=False)\n",
        "\n",
        "print(\"Data merged successfully. Saved to 'merged_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "kOwfk8MO79y5",
        "outputId": "2f6b4fa7-74ea-4ddb-8961-4d2083c8ea42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/usr/realestate data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c738b30d347c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Loop through all files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/realestate data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('merged_data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "pCOANan38OTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "KFDqD11I8qCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "4rG0VBPY8vur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ccMJGqu082HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['property_url','mls','mls_id','text','primary_photo','alt_photos','broker_phone','latitude','longitude'], inplace=True)"
      ],
      "metadata": {
        "id": "pgjexEeqBbby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hcUcINsdBbY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "z1pSwyBRPTlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "vMo2BjMQBbWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove duplicates\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "df\n"
      ],
      "metadata": {
        "id": "eHlP2gCPBbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "5e787hzHBbMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a instance of label Encoder.\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(df['status'])\n",
        "label = le.fit_transform(df['style'])\n",
        "label = le.fit_transform(df['full_street_line'])\n",
        "label = le.fit_transform(df['street'])\n",
        "label = le.fit_transform(df['unit'])\n",
        "label = le.fit_transform(df['city'])\n",
        "label = le.fit_transform(df['state'])\n",
        "label = le.fit_transform(df['zip_code'])\n",
        "label = le.fit_transform(df['beds'])\n",
        "label = le.fit_transform(df['full_baths'])\n",
        "label = le.fit_transform(df['half_baths'])\n",
        "label = le.fit_transform(df['sqft'])\n",
        "label = le.fit_transform(df['year_built'])\n",
        "label = le.fit_transform(df['days_on_mls'])\n",
        "label = le.fit_transform(df['list_price'])\n",
        "label = le.fit_transform(df['list_date'])\n",
        "label = le.fit_transform(df['sold_price'])\n",
        "label = le.fit_transform(df['last_sold_date'])\n",
        "label = le.fit_transform(df['assessed_value'])\n",
        "label = le.fit_transform(df['estimated_value'])\n",
        "label = le.fit_transform(df['lot_sqft'])\n",
        "label = le.fit_transform(df['price_per_sqft'])\n",
        "label = le.fit_transform(df['neighborhoods'])\n",
        "label = le.fit_transform(df['county'])\n",
        "label = le.fit_transform(df['fips_code'])\n",
        "label = le.fit_transform(df['stories'])\n",
        "label = le.fit_transform(df['hoa_fee'])\n",
        "label = le.fit_transform(df['parking_garage'])\n",
        "label = le.fit_transform(df['agent'])\n",
        "label = le.fit_transform(df['agent_phones'])\n",
        "label = le.fit_transform(df['agent_email'])\n",
        "label = le.fit_transform(df['broker'])\n",
        "label = le.fit_transform(df['broker_website'])\n",
        "label = le.fit_transform(df['nearby_schools'])\n",
        "\n",
        "# printing label\n",
        "label\n"
      ],
      "metadata": {
        "id": "VUf3Sn6jBbJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(\"status\", axis=1, inplace=True)\n",
        "#df.drop(\"style\", axis=1, inplace=True)\n",
        "df.drop(\"full_street_line\", axis=1, inplace=True)\n",
        "df.drop(\"unit\", axis=1, inplace=True)\n",
        "df.drop(\"city\", axis=1, inplace=True)\n",
        "df.drop(\"state\", axis=1, inplace=True)\n",
        "df.drop(\"zip_code\", axis=1, inplace=True)\n",
        "df.drop(\"beds\", axis=1, inplace=True)\n",
        "df.drop(\"full_baths\", axis=1, inplace=True)\n",
        "df.drop(\"half_baths\", axis=1, inplace=True)\n",
        "df.drop(\"sqft\", axis=1, inplace=True)\n",
        "df.drop(\"year_built\", axis=1, inplace=True)\n",
        "df.drop(\"days_on_mls\", axis=1, inplace=True)\n",
        "df.drop(\"list_price\", axis=1, inplace=True)\n",
        "df.drop(\"list_date\", axis=1, inplace=True)\n",
        "df.drop(\"sold_price\", axis=1, inplace=True)\n",
        "df.drop(\"last_sold_date\", axis=1, inplace=True)\n",
        "df.drop(\"assessed_value\", axis=1, inplace=True)\n",
        "df.drop(\"estimated_value\", axis=1, inplace=True)\n",
        "df.drop(\"lot_sqft\", axis=1, inplace=True)\n",
        "df.drop(\"price_per_sqft\", axis=1, inplace=True)\n",
        "df.drop(\"neighborhoods\", axis=1, inplace=True)\n",
        "df.drop(\"county\", axis=1, inplace=True)\n",
        "df.drop(\"fips_code\", axis=1, inplace=True)\n",
        "df.drop(\"stories\", axis=1, inplace=True)\n",
        "df.drop(\"hoa_fee\", axis=1, inplace=True)\n",
        "df.drop(\"parking_garage\", axis=1, inplace=True)\n",
        "df.drop(\"agent\", axis=1, inplace=True)\n",
        "df.drop(\"agent_email\", axis=1, inplace=True)\n",
        "df.drop(\"agent_phones\", axis=1, inplace=True)\n",
        "df.drop(\"broker\", axis=1, inplace=True)\n",
        "df.drop(\"broker_website\", axis=1, inplace=True)\n",
        "df.drop(\"nearby_schools\", axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6uNtyW9UBbCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Appending the array to our dataFrame\n",
        "# with column name 'Purchased'\n",
        "df[\"status\"] = label\n",
        "df[\"style\"] = label\n",
        "df[\"full_street_line\"] = label\n",
        "df[\"street\"] = label\n",
        "df[\"unit\"] = label\n",
        "df[\"city\"] = label\n",
        "df[\"state\"] = label\n",
        "df[\"zip_code\"] = label\n",
        "df[\"beds\"] = label\n",
        "df[\"full_baths\"] = label\n",
        "df[\"half_baths\"] = label\n",
        "df[\"sqft\"] = label\n",
        "df[\"year_built\"] = label\n",
        "df[\"days_on_mls\"] = label\n",
        "df[\"list_price\"] = label\n",
        "df[\"list_date\"] = label\n",
        "df[\"sold_price\"] = label\n",
        "df[\"last_sold_price\"] = label\n",
        "df[\"assessed_value\"] = label\n",
        "df[\"estimated_value\"] = label\n",
        "df[\"loft_sqft\"] = label\n",
        "df[\"price_per_sqft\"] = label\n",
        "df[\"neighborhoods\"] = label\n",
        "df[\"county\"] = label\n",
        "df[\"fips_code\"] = label\n",
        "df[\"stories\"] = label\n",
        "df[\"hoa_fee\"] = label\n",
        "df[\"parking_garage\"] = label\n",
        "df[\"agent\"] = label\n",
        "df[\"agent_email\"] = label\n",
        "df[\"agent_phones\"] = label\n",
        "df[\"broker\"] = label\n",
        "df[\"broker_website\"] = label\n",
        "df[\"nearby_schools\"] = label"
      ],
      "metadata": {
        "id": "VolXLyqoBa-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "uev7qJcVBagY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Using Seaborn to create a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)\n",
        "\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MSPL1reslX0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Seaborn style\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_columns = df.select_dtypes(include=[\"int64\"]).columns\n",
        "\n",
        "# Plot distribution of each numerical feature\n",
        "plt.figure(figsize=(14, len(numerical_columns) * 3))\n",
        "for idx, feature in enumerate(numerical_columns, 1):\n",
        "    plt.subplot(len(numerical_columns), 2, idx)\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f\"{feature} | Skewness: {round(df[feature].skew(), 2)}\")\n",
        "\n",
        "# Adjust layout and show plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f1-dcyQJuJGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Using Seaborn to create a swarm plot\n",
        "sns.swarmplot(x=\"status\", y=\"street\", data=df, palette='viridis')\n",
        "\n",
        "plt.title('Swarm Plot for syle and street')\n",
        "plt.xlabel('status')\n",
        "plt.ylabel('street')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sq6q3UF4unWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plotting the line chart\n",
        "fig = px.line(df, x=\"nearby\", y=\"parking_garage\")\n",
        "\n",
        "# showing the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "krt8wL1P0t2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing machine learning algorithms:"
      ],
      "metadata": {
        "id": "DFEfCxDLEoT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "rxTZZxkdEvMP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "T5mwgOijPY2c"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSgHTvsQPvU3",
        "outputId": "1d0f2045-eaf8-41f7-bbfd-b9df8a0d7a10"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[23, 23, '23', ..., 23, 23, 23],\n",
              "       [62, 62, '62', ..., 62, 62, 62],\n",
              "       [30, 30, '30', ..., 30, 30, 30],\n",
              "       ...,\n",
              "       [13, 13, '13', ..., 13, 13, 13],\n",
              "       [2, 2, '2', ..., 2, 2, 2],\n",
              "       [62, 62, '62', ..., 62, 62, 62]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aCo-RvmP6Py",
        "outputId": "a1e29577-7a77-4662-d641-466aa229c2f5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[32, 32, '32', ..., 32, 32, 32],\n",
              "       [58, 58, '58', ..., 58, 58, 58],\n",
              "       [22, 22, '22', ..., 22, 22, 22],\n",
              "       ...,\n",
              "       [10, 10, '10', ..., 10, 10, 10],\n",
              "       [38, 38, '38', ..., 38, 38, 38],\n",
              "       [32, 32, '32', ..., 32, 32, 32]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7DdLpTfQMrf",
        "outputId": "af1b13f0-8d62-4b4d-aa3f-0af2f23c912e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([23, 62, 30, ..., 13,  2, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKxDqkxQPin",
        "outputId": "008bf00d-a7e6-442e-af22-17b011ed4d5a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32, 58, 22, 22, 57, 49,  4,  7, 30, 22, 13, 55, 23, 58, 32, 32, 39,\n",
              "       22, 45, 23, 32, 32, 38, 55, 32, 57, 55, 14, 35, 26, 21,  2, 22,  7,\n",
              "       62, 55, 44, 30, 55, 32, 45, 35, 40, 45,  7, 32, 57, 32, 29, 22,  7,\n",
              "        7, 37, 32, 23, 16, 57, 57, 32,  3, 26, 15, 32, 22, 49, 40, 55,  4,\n",
              "       35,  4, 37, 58, 33, 23, 38, 55, 22,  7, 20, 55, 32, 36, 15, 13, 38,\n",
              "       26, 29,  7, 13, 58, 18, 20, 13, 50, 27, 58, 34, 35, 32, 33, 13,  7,\n",
              "       58, 26, 39, 32, 21, 35, 32, 57, 49, 34, 23,  9, 35, 13, 17, 32, 32,\n",
              "       32, 50, 38, 55, 23, 49, 32,  3, 60, 23, 15, 26, 55, 22, 57, 23, 49,\n",
              "        4, 52, 44, 58, 49, 23, 40, 14, 29, 62, 55, 35, 39, 30, 20, 29, 14,\n",
              "       17, 13, 40, 38, 53, 40, 33, 15, 15,  0, 30, 23, 14, 32, 55, 34, 30,\n",
              "       15, 17, 38, 35, 29,  4, 45, 11, 14,  7, 54,  7, 38, 62, 62, 30, 34,\n",
              "       44, 58, 53, 61, 34, 35, 40, 23, 17, 32, 29, 34, 40, 35, 39, 32, 55,\n",
              "       11,  1, 16, 20, 11, 26,  4,  7, 29, 32, 35, 11, 44, 26, 35, 11,  4,\n",
              "       32, 13, 15, 34, 22, 26, 17, 17, 38, 27, 50, 32, 23, 22, 22,  7, 21,\n",
              "       22, 23, 20, 18, 11, 44,  7, 44, 23,  7, 35, 49, 40, 61, 39,  4, 20,\n",
              "        7, 29, 35,  3, 58, 35, 40, 22,  4, 37, 35, 57, 12, 30, 23, 32, 35,\n",
              "       62, 26, 60, 35, 62, 35, 40,  7, 32, 40, 35, 32, 55, 36, 33, 14, 57,\n",
              "       55, 26, 39, 22, 15, 58, 23, 23, 17, 30, 17,  7, 23, 58, 55, 38, 14,\n",
              "       30, 32, 44, 58, 49, 58, 20, 58,  0, 40, 22,  0, 29, 38,  7, 38, 40,\n",
              "        7, 22, 39, 13, 23, 44, 22, 35, 15, 45, 15, 22, 27, 34, 22, 62, 20,\n",
              "       15, 23, 35, 15, 28,  2, 35, 35, 15,  7, 44, 32, 25, 26, 32, 32, 57,\n",
              "       58, 22, 17, 22, 32, 34, 35, 33, 20, 37, 36, 35, 22, 11,  7, 41, 32,\n",
              "        4, 11, 22, 14, 32,  4, 15, 57, 32, 35, 35, 22, 55, 26, 29, 32, 55,\n",
              "       15, 13, 57, 22, 39, 23, 29,  7, 44, 33, 33, 17, 11,  3, 30, 29, 44,\n",
              "       38, 29, 32,  4, 40, 29,  7, 17, 49, 34, 27, 35, 35, 40, 57, 55, 40,\n",
              "       30, 32, 22, 57, 14, 14, 35, 57, 35, 40, 23, 32, 39, 56, 57, 30, 32,\n",
              "       22, 14, 32, 47,  7, 28, 20, 62, 33, 15, 35, 17, 55, 13, 55, 44, 55,\n",
              "       23, 30, 23, 13, 55, 57, 30, 40, 39, 55, 40, 22, 32, 44, 14, 53, 21,\n",
              "       40, 40, 23, 23, 53,  7, 58, 45, 22, 10, 17,  4, 30, 24,  7, 32,  7,\n",
              "        4, 10, 30, 20,  4, 23, 40,  0, 23, 23, 11, 14, 57, 10, 38, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LcjO39-VPc8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "KkjTw31AXD3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42,max_depth = None)\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "P0Fp-HUSQV_2",
        "outputId": "212cf0df-5188-46ea-bc36-72a5f2d4802d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_preds = rf.predict(X)\n",
        "print(\"Classification Report\")\n",
        "print(\"Accuracy : {}\".format(accuracy_score(y, rf_preds)))\n",
        "print(\"Precision : {}\".format(precision_score(y, rf_preds, average = 'macro')))\n",
        "print(\"Recall : {}\".format(recall_score(y, rf_preds,average = 'macro')))\n",
        "print(\"F1 Score : {}\".format(f1_score(y, rf_preds,average = 'macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNPeZaBwTvAW",
        "outputId": "5d8c4975-0089-4ae4-ffb2-ccea4d1d31e4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "Accuracy : 0.9996067636649626\n",
            "Precision : 0.9836860670194003\n",
            "Recall : 0.9841269841269841\n",
            "F1 Score : 0.9839034205231388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train, y_train)\n",
        "knn_preds = knn.predict(X)"
      ],
      "metadata": {
        "id": "m09zxFIIW0jn"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(\"Accuracy : {}\".format(accuracy_score(y, knn_preds)))\n",
        "print(\"Precision : {}\".format(precision_score(y, knn_preds, average = 'macro')))\n",
        "print(\"Recall : {}\".format(recall_score(y, knn_preds,average = 'macro')))\n",
        "print(\"F1 Score : {}\".format(f1_score(y, knn_preds,average = 'macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPNklT4uX1QI",
        "outputId": "cab07249-aa09-4bf2-8b1f-8e2566723b46"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "Accuracy : 0.9720802202123476\n",
            "Precision : 0.6634698295355259\n",
            "Recall : 0.6825396825396826\n",
            "F1 Score : 0.6723321131138914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_data_validation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QmTiycPSY7Ls",
        "outputId": "bb45978e-3688-437c-af97-74f813d77beb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_data_validation\n",
            "  Downloading tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.25.2)\n",
            "Collecting pandas<2,>=1.0 (from tensorflow_data_validation)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<11,>=10 (from tensorflow_data_validation)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2.2 (from tensorflow_data_validation)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.16.0)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-metadata<1.16,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.15.0)\n",
            "Collecting tfx-bsl<1.16,>=1.15.1 (from tensorflow_data_validation)\n",
            "  Downloading tfx_bsl-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation)\n",
            "  Downloading apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (3.20.3)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.64.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.2.1)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (24.1)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading pymongo-4.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (669 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.1/669.1 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.23.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2023.4)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading redis-5.0.6-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2024.5.15)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.12.2)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.6)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (5.3.3)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.11.1)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.15.2)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_pubsub-2.21.5-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_pubsublite-1.10.0-py2.py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-storage<3,>=2.14.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_storage-2.17.0-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.21.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.25.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.3.3)\n",
            "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_bigtable-2.24.0-py2.py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.7/373.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_spanner-3.47.0-py2.py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.6/384.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_dlp-3.18.0-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.13.3)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_videointelligence-2.13.3-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.4/240.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_vision-3.7.2-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.6/459.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.10-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.56.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.37.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation) (2.15.0)\n",
            "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.16,>=1.15.1->tensorflow_data_validation)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.16,>=1.15.1->tensorflow_data_validation)\n",
            "  Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.43.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.63.1)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.16,>=1.15.1->tensorflow_data_validation)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.0.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.7.4)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.7.1)\n",
            "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.13.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.48.2)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.5.0)\n",
            "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.5.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.1.2)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.18.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2024.6.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of tensorflow-serving-api to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.16,>=1.15.1->tensorflow_data_validation)\n",
            "  Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting tensorflow<2.16,>=2.15 (from tensorflow_data_validation)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes~=0.3.1 (from tensorflow<2.16,>=2.15->tensorflow_data_validation)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (1.3.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.18.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tensorflow_data_validation) (3.2.2)\n",
            "Building wheels for collected packages: pyfarmhash, crcmod, dill, google-apitools, hdfs, pyjsparser, docopt\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88657 sha256=114932ea34abfbdd2393c37e832f6a6e08feb2a29f9996a8481c04a86eee52e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31404 sha256=68f3c8c03814256fc512e6df287a44498a4cccc7763aa12aa7ecdf35b2308d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78541 sha256=8ee6d7f4ccd0d5d7cdd13cce7bef57285b220caff02927d0764146ed1885d8b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131015 sha256=de9185b3ec15d799898e82cb1eb0d8204ae91ba9eec035d36cd8d01122e3c955\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=2ea7ad9c8dba15fb73d253722f6db7635860776623542734d43f438e21ec14b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=55c335d786c9022e822f9d4a6fd7f08da4f50538845ac481b2477a29107c2144\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6c7991770708d1e038ca1cc8e5bafafd2debc215f99515369f6cd615d4276da9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built pyfarmhash crcmod dill google-apitools hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, pyfarmhash, docopt, crcmod, zstandard, uritemplate, redis, pyarrow, overrides, orjson, objsize, ml-dtypes, js2py, grpc-interceptor, fasteners, fastavro, dnspython, dill, pymongo, pandas, hdfs, google-apitools, google-api-core, google-api-python-client, apache-beam, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-dlp, google-cloud-bigtable, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow_data_validation\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.1\n",
            "    Uninstalling google-api-core-2.11.1:\n",
            "      Successfully uninstalled google-api-core-2.11.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 10.0.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.56.0 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 google-api-core-2.19.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigtable-2.24.0 google-cloud-dlp-3.18.0 google-cloud-pubsub-2.21.5 google-cloud-pubsublite-1.10.0 google-cloud-recommendations-ai-0.10.10 google-cloud-spanner-3.47.0 google-cloud-storage-2.17.0 google-cloud-videointelligence-2.13.3 google-cloud-vision-3.7.2 grpc-interceptor-0.15.4 hdfs-2.7.3 js2py-0.74 ml-dtypes-0.3.2 objsize-0.7.0 orjson-3.10.5 overrides-7.7.0 pandas-1.5.3 pyarrow-10.0.1 pyfarmhash-0.3.2 pyjsparser-2.7.1 pymongo-4.7.3 redis-5.0.6 tensorflow-2.15.1 tensorflow-serving-api-2.15.1 tensorflow_data_validation-1.15.1 tfx-bsl-1.15.1 uritemplate-3.0.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pandas",
                  "pyarrow"
                ]
              },
              "id": "68deea2b303e48e6bfa0d28b17639580"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}